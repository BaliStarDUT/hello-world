# cgroup 原理介绍与资源配额的物理逻辑实现

`cgroup`（Control Groups，控制组）是 Linux 内核提供的一种**资源隔离与限制机制**，核心目标是对进程组（而非单个进程）的 CPU、内存、IO、网络等硬件资源进行精细化分配、限制和监控。它是容器（Docker、K8s）、轻量级虚拟机（LXC）等技术实现资源隔离的底层核心依赖，确保不同任务在共享宿主机硬件时，不会因资源争抢导致系统不稳定。


## 一、cgroup 核心原理：四大核心概念与分层结构

要理解 cgroup 的资源配额逻辑，首先需要掌握其核心组成部分和工作模型：


### 1. 四大核心概念
cgroup 的设计围绕以下四个关键概念展开，共同构成资源管控的基础：

| 概念          | 作用说明                                                                 |
|---------------|--------------------------------------------------------------------------|
| **任务（Task）** | 指 Linux 中的单个进程（PID 标识），是 cgroup 资源管控的最小“被管控单元”。一个任务可以加入多个 cgroup（不同子系统）。 |
| **控制组（Cgroup）** | 一组“任务”的集合，是资源管控的“逻辑单元”。同一控制组内的所有任务共享相同的资源配额（如 CPU 使用率上限、内存限制）。 |
| **子系统（Subsystem）** | 对应一种具体的资源类型，是 cgroup 实现“资源管控”的功能模块。每个子系统负责一类资源的限制（如 `cpu` 子系统管 CPU，`memory` 子系统管内存）。 |
| **层级（Hierarchy）** | 由多个控制组构成的“树状结构”，用于实现资源的**继承与优先级**。层级与子系统绑定（一个层级可绑定多个子系统，一个子系统只能绑定一个层级），树的父子节点间可继承资源配置（如子组的 CPU 配额不能超过父组）。 |


### 2. 核心工作模型：“层级-子系统-控制组-任务”的绑定关系
cgroup 的资源管控逻辑通过以下绑定规则实现，确保资源隔离的准确性和灵活性：
1. **一个层级可绑定多个子系统**：例如，一个层级同时绑定 `cpu` 和 `cpuacct`（CPU 统计）子系统，实现对进程组的 CPU 限制+使用统计。
2. **一个子系统只能绑定一个层级**：避免资源规则冲突（如 `memory` 子系统若绑定两个层级，进程可能同时受两个内存限制，逻辑混乱）。
3. **一个任务可加入多个层级的控制组**：例如，一个进程可加入“CPU 控制组”（受 CPU 限制）和“内存控制组”（受内存限制），分别对应不同层级。
4. **一个层级内的任务只能属于一个控制组**：确保同一资源类型的限制唯一（如一个进程在 `cpu` 子系统绑定的层级中，只能属于一个 CPU 控制组，避免多个 CPU 配额冲突）。


## 二、cgroup 资源配额的物理逻辑实现：从内核到硬件的管控链路

cgroup 本身不直接“控制硬件”，而是通过**内核子系统**对接硬件驱动/调度器，将“资源配额规则”转化为对硬件资源的实际管控。不同子系统（CPU、内存、IO 等）的实现逻辑不同，以下以最常用的 **CPU**、**内存**、**块设备 IO** 为例，详解其物理层面的配额实现原理。


### 1. CPU 资源配额：控制“时间片分配”，限制进程占用核心的时长
CPU 的核心是“时间共享”（多个进程轮流使用 CPU 核心），cgroup 的 `cpu` 子系统通过**控制进程获得的 CPU 时间片比例**，实现配额限制。核心机制包括 `CFS 带宽控制` 和 `CPU 亲和性`。

#### （1）核心技术：CFS 带宽控制（CFS Bandwidth Control）
Linux 内核的 CPU 调度器默认是 **CFS（Completely Fair Scheduler，完全公平调度器）**，其核心是给每个进程分配“公平的时间片”。cgroup 通过扩展 CFS，为控制组设置“时间配额上限”：
- **关键配置参数**（通过 `/sys/fs/cgroup/cpu/<cgroup>/` 下的文件配置）：
  - `cpu.cfs_period_us`：调度周期（单位：微秒，默认 100000，即 100ms）。表示内核每 `cfs_period_us` 时间，重新分配一次 CPU 时间片。
  - `cpu.cfs_quota_us`：该控制组在一个周期内可使用的最大 CPU 时间（单位：微秒）。

- **物理逻辑举例**：
  若配置 `cpu.cfs_period_us=100000`（100ms）、`cpu.cfs_quota_us=50000`（50ms），则该控制组内的所有进程在每 100ms 内，最多只能占用 CPU 核心 50ms——即**CPU 使用率上限为 50%**。
  - 若宿主机是 4 核 CPU，想让控制组使用“1 个核心的 100%”，则配置 `cpu.cfs_quota_us=100000`（100ms）、`cpu.cfs_period_us=100000`（100ms），此时配额为 `100ms/100ms = 1 核`。
  - 若想限制为“2 个核心的 50%”，则配置 `cpu.cfs_quota_us=100000`（100ms）、`cpu.cfs_period_us=100000`（100ms），结合 `cpu.shares`（权重）进一步分配。

- **内核实现逻辑**：
  1. 内核为每个 CPU 核心维护一个 `CFS 运行队列`，记录该核心上待调度的进程。
  2. 每个控制组有一个“时间计数器”，记录该组在当前 `cfs_period_us` 内已使用的 CPU 时间。
  3. 当控制组的已用时间达到 `cfs_quota_us` 时，内核会将该组内的所有进程标记为“限流状态”，暂时移出 CFS 运行队列，直到下一个调度周期开始（时间计数器重置）。


#### （2）辅助机制：CPU 亲和性（CPU Affinity）
通过 `cpu.cpus` 参数限制控制组内的进程只能在指定的 CPU 核心上运行，实现“物理核心级的隔离”：
- 例如，配置 `cpu.cpus=0-1`，则该控制组的进程只能使用第 0、1 号 CPU 核心，无法占用其他核心的资源，避免对其他核心上的任务造成干扰。


### 2. 内存资源配额：控制“物理内存/交换分区的使用上限”，避免内存溢出
内存是“独占性资源”（进程占用内存后，其他进程无法使用），cgroup 的 `memory` 子系统通过**设置内存使用上限**，防止单个控制组耗尽宿主机内存。核心机制包括“内存限制”和“OOM 保护”。

#### （1）核心技术：内存使用上限控制
- **关键配置参数**（通过 `/sys/fs/cgroup/memory/<cgroup>/` 下的文件配置）：
  - `memory.limit_in_bytes`：该控制组可使用的**物理内存上限**（如 `2147483648` 表示 2GB）。
  - `memory.swappiness`：控制进程使用交换分区（Swap）的倾向（0-100，值越高越倾向用 Swap）。
  - `memory.memsw.limit_in_bytes`：物理内存+交换分区的**总使用上限**（避免 Swap 被耗尽）。

- **物理逻辑举例**：
  配置 `memory.limit_in_bytes=2G`、`memory.memsw.limit_in_bytes=4G`，则该控制组内的所有进程：
  1. 物理内存使用量不能超过 2GB；
  2. 若物理内存不足，可使用 Swap，但物理内存+Swap 的总占用不能超过 4GB；
  3. 当总占用达到 4GB 时，内核会阻止该组进程继续申请内存（返回 `ENOMEM` 错误）。

- **内核实现逻辑**：
  1. Linux 内核通过“页表”管理内存（每个进程有独立页表，记录虚拟地址到物理地址的映射）。
  2. `memory` 子系统为每个控制组维护一个“内存使用计数器”，实时统计该组所有进程的物理内存/swap 占用量。
  3. 当进程申请内存时，内核先检查其所属控制组的计数器是否已达上限：
     - 未达上限：允许申请，更新计数器；
     - 已达上限：拒绝申请（返回错误），若开启 OOM 杀手（`memory.oom_control`），则会杀死组内“内存占用最高”的进程，释放资源。


### 3. 块设备 IO 资源配额：控制“IO 读写速率/请求数”，避免磁盘争抢
块设备（如硬盘、SSD）的 IO 资源是“共享瓶颈”，cgroup 的 `blkio` 子系统通过**限制控制组对块设备的读写速率或请求数量**，避免单个任务占满磁盘 IO。核心机制包括“权重控制”和“速率限制”。

#### （1）核心技术 1：IO 权重控制（按比例分配 IO 带宽）
- **适用场景**：多个控制组共享同一块设备，按比例分配 IO 资源（如 A 组占 60%，B 组占 40%）。
- **关键配置参数**：
  - `blkio.weight`：默认权重（100-1000，默认 500），适用于所有块设备。
  - `blkio.weight_device`：针对特定块设备的权重（如 `8:0 600`，表示对主设备号 8、次设备号 0 的磁盘，权重设为 600）。

- **物理逻辑举例**：
  控制组 A 配置 `blkio.weight_device=8:0 600`，控制组 B 配置 `blkio.weight_device=8:0 400`，则当 A、B 同时对 `8:0` 磁盘发起 IO 时，内核会按 6:4 的比例分配 IO 时间片——A 组获得 60% 的 IO 带宽，B 组获得 40%。


#### （2）核心技术 2：IO 速率限制（固定读写速率上限）
- **适用场景**：限制单个控制组的 IO 速率（如限制为 100MB/s，避免影响其他任务）。
- **关键配置参数**：
  - `blkio.throttle.read_bps_device`：特定块设备的**读速率上限**（如 `8:0 104857600`，表示对 `8:0` 磁盘，读速率不超过 100MB/s，1MB=1048576 字节）。
  - `blkio.throttle.write_bps_device`：特定块设备的**写速率上限**。

- **内核实现逻辑**：
  1. 内核为每个块设备维护一个“IO 调度队列”，控制组的 IO 请求会先进入对应队列。
  2. 对于权重控制：内核按控制组的权重比例，从不同队列中“轮询”取出 IO 请求发送给磁盘驱动。
  3. 对于速率控制：内核为每个控制组的 IO 队列维护一个“速率计数器”，若单位时间内的 IO 量达到上限，则暂停该队列的请求发送，直到下一个时间窗口（如 100ms）。


## 三、cgroup 的挂载与配置：用户态如何操作内核资源管控
cgroup 作为内核特性，通过 **tmpfs 文件系统** 暴露给用户态（即 `/sys/fs/cgroup/` 目录），用户可通过“创建目录（控制组）、修改文件（配置参数）”的方式操作 cgroup，无需编写内核代码。


### 1. cgroup 的默认挂载
Linux 系统启动时，会自动将所有 cgroup 子系统挂载到 `/sys/fs/cgroup/` 目录，每个子系统对应一个子目录（如 `cpu/`、`memory/`、`blkio/`）：
```bash
# 查看 cgroup 挂载情况
mount | grep cgroup
# 输出示例（部分）：
# tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
# cgroup on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu)
# cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
```


### 2. 手动配置 cgroup 示例（以 CPU 限制为例）
通过手动操作 `/sys/fs/cgroup/cpu/` 目录，创建控制组并限制 CPU 使用率：
```bash
# 1. 创建一个名为 "my-cgroup" 的 CPU 控制组（本质是创建目录）
sudo mkdir /sys/fs/cgroup/cpu/my-cgroup

# 2. 配置 CPU 配额：周期 100ms，配额 50ms（即 CPU 使用率上限 50%）
sudo echo 100000 > /sys/fs/cgroup/cpu/my-cgroup/cpu.cfs_period_us
sudo echo 50000 > /sys/fs/cgroup/cpu/my-cgroup/cpu.cfs_quota_us

# 3. 将当前进程（PID=$$）加入该控制组
sudo echo $$ > /sys/fs/cgroup/cpu/my-cgroup/cgroup.procs

# 4. 验证：当前进程的 CPU 使用率会被限制在 50% 以内
while true; do :; done  # 运行一个占满 CPU 的循环
```


## 四、cgroup 与容器的关系：容器资源隔离的底层依赖
Docker、K8s 等容器技术的“资源隔离”本质是**对 cgroup 的封装**：
- 当创建一个容器时，容器引擎（如 Docker）会自动在 `/sys/fs/cgroup/` 的各个子系统目录下，为该容器创建一个独立的控制组（如 `docker/<容器ID>/`）。
- 根据用户指定的资源参数（如 `docker run --cpus 0.5 --memory 1G`），容器引擎会自动修改该控制组的 `cpu.cfs_quota_us`、`memory.limit_in_bytes` 等配置文件。
- 容器内的所有进程（如 `PID=1` 的初始化进程及子进程），会被自动加入该控制组，从而受 cgroup 的资源配额限制。


## 总结
cgroup 的核心价值是**将内核的资源管控能力通过“文件系统接口”暴露给用户态**，其资源配额的物理逻辑本质是：
1. **针对不同资源类型（CPU/内存/IO），通过专用子系统对接内核调度器/驱动**；
2. **将用户配置的“配额规则”（如 CPU 周期/内存上限/IO 速率）转化为内核可执行的“调度策略”**；
3. **实时统计进程组的资源使用量，当达到配额上限时，通过“限流、拒绝申请、杀死进程”等方式强制管控**。

正是基于 cgroup，Linux 才实现了“轻量级资源隔离”，为容器、云原生等技术奠定了底层基础。